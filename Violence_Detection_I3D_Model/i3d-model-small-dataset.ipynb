{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10713139,"sourceType":"datasetVersion","datasetId":6640233}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport torch\nimport random\nimport numpy as np\nimport shutil\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom transformers import ViTModel\nfrom tqdm import tqdm\n\n# Paths in Kaggle format\ndata_root = \"/kaggle/input/violence-detection-dataset/violence-detection-dataset\"\noutput_root = \"/kaggle/working/frames\"\ncategories = [\"violent\", \"non-violent\"]\nFRAME_RATE = 5  # Extract every 5th frame\nSPLIT_RATIO = 0.8  # 80% train, 20% validation\n\n# Ensure output directory exists\nos.makedirs(output_root, exist_ok=True)\n\n# Frame extraction function\ndef extract_frames(video_path, output_folder, frame_rate=5):\n    cap = cv2.VideoCapture(video_path)\n    frame_count = 0\n    \n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n        if frame_count % frame_rate == 0:\n            frame_filename = os.path.join(output_folder, f\"{os.path.basename(video_path)}_frame{frame_count}.jpg\")\n            cv2.imwrite(frame_filename, frame)\n        frame_count += 1\n    \n    cap.release()\n\n# Extract frames for both categories\nfor category in categories:\n    category_path = os.path.join(data_root, category)\n    output_category_path = os.path.join(output_root, category)\n    os.makedirs(output_category_path, exist_ok=True)\n    \n    for cam_folder in os.listdir(category_path):\n        cam_path = os.path.join(category_path, cam_folder)\n        if not os.path.isdir(cam_path):\n            continue  # Skip if not a folder\n        \n        for video_file in tqdm(os.listdir(cam_path), desc=f\"Processing {category}/{cam_folder}\"):\n            video_path = os.path.join(cam_path, video_file)\n            if video_file.endswith(\".mp4\"):\n                extract_frames(video_path, output_category_path, FRAME_RATE)\n\n# Define dataset class\nclass ViolenceDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.images = []\n        self.labels = []\n        \n        for category in categories:\n            category_path = os.path.join(root_dir, category)\n            label = 1 if category == \"violent\" else 0\n            for img_file in os.listdir(category_path):\n                self.images.append(os.path.join(category_path, img_file))\n                self.labels.append(label)\n        \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, idx):\n        img_path = self.images[idx]\n        image = cv2.imread(img_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        if self.transform:\n            image = self.transform(image)\n        return image, self.labels[idx]\n\n# Define transformations\ntransform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Create dataset and split\nfull_dataset = ViolenceDataset(output_root, transform=transform)\nif len(full_dataset) == 0:\n    raise ValueError(\"Dataset is empty. Please check the data directory structure and extracted frames.\")\ntrain_size = int(SPLIT_RATIO * len(full_dataset))\nval_size = len(full_dataset) - train_size\ntrain_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n\n# Create dataloaders\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n\n# Define ViViT model\nclass ViViTClassifier(nn.Module):\n    def __init__(self, num_classes=2):\n        super(ViViTClassifier, self).__init__()\n        self.vit = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n        self.fc = nn.Linear(768, num_classes)\n    \n    def forward(self, x):\n        outputs = self.vit(x).last_hidden_state[:, 0, :]\n        return self.fc(outputs)\n\n# Initialize model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = ViViTClassifier().to(device)\n\n# Define loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n\n# Training loop\nEPOCHS = 5\nfor epoch in range(EPOCHS):\n    model.train()\n    running_loss = 0.0\n    \n    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    \n    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}\")\n    \nprint(\"Training complete!\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:12:55.161658Z","iopub.execute_input":"2025-02-10T16:12:55.161994Z","iopub.status.idle":"2025-02-10T16:24:59.355020Z","shell.execute_reply.started":"2025-02-10T16:12:55.161969Z","shell.execute_reply":"2025-02-10T16:24:59.353890Z"}},"outputs":[{"name":"stderr","text":"Processing violent/cam1: 100%|██████████| 115/115 [02:13<00:00,  1.16s/it]\nProcessing violent/cam2: 100%|██████████| 115/115 [02:09<00:00,  1.13s/it]\nProcessing non-violent/cam1: 100%|██████████| 60/60 [01:06<00:00,  1.10s/it]\nProcessing non-violent/cam2: 100%|██████████| 60/60 [01:02<00:00,  1.04s/it]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"907e06404ae04e8e874214964067b1c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfd89efc14044f4d86787988fa39f7eb"}},"metadata":{}},{"name":"stderr","text":"Epoch 1/5: 100%|██████████| 301/301 [04:53<00:00,  1.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Loss: 0.1750\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5:  11%|█         | 33/301 [00:35<04:46,  1.07s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-e0d586e5364c>\u001b[0m in \u001b[0;36m<cell line: 124>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":5},{"cell_type":"code","source":"!pip install decord","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T11:33:16.653664Z","iopub.execute_input":"2025-04-05T11:33:16.654017Z","iopub.status.idle":"2025-04-05T11:33:20.000759Z","shell.execute_reply.started":"2025-04-05T11:33:16.653988Z","shell.execute_reply":"2025-04-05T11:33:19.999751Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: decord in /usr/local/lib/python3.10/dist-packages (0.6.0)\nRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from decord) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->decord) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.14.0->decord) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.14.0->decord) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.14.0->decord) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.14.0->decord) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.14.0->decord) (2024.2.0)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport cv2\nimport torch\nimport random\nimport numpy as np\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom transformers import VideoMAEModel\nfrom decord import VideoReader, cpu\nfrom tqdm import tqdm\n\n# Paths in Kaggle format\ndata_root = \"/kaggle/input/violence-detection-dataset/violence-detection-dataset\"\ncategories = [\"violent\", \"non-violent\"]\nSPLIT_RATIO = 0.8  # 80% train, 20% validation\nNUM_FRAMES = 16  # Number of frames per video clip\n\n# Define dataset class\nclass ViolenceDataset(Dataset):\n    def __init__(self, root_dir, transform=None, num_frames=16):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.num_frames = num_frames\n        self.videos = []\n        self.labels = []\n        \n        for category in categories:\n            category_path = os.path.join(root_dir, category)\n            label = 1 if category == \"violent\" else 0\n            for cam_folder in os.listdir(category_path):\n                cam_path = os.path.join(category_path, cam_folder)\n                if not os.path.isdir(cam_path):\n                    continue  # Skip if not a folder\n                for video_file in os.listdir(cam_path):\n                    if video_file.endswith(\".mp4\"):\n                        self.videos.append(os.path.join(cam_path, video_file))\n                        self.labels.append(label)\n    \n    def __len__(self):\n        return len(self.videos)\n    \n    def __getitem__(self, idx):\n        video_path = self.videos[idx]\n        vr = VideoReader(video_path, ctx=cpu(0))\n        total_frames = len(vr)\n        frame_indices = np.linspace(0, total_frames - 1, self.num_frames, dtype=int)\n        frames = [vr[i].asnumpy() for i in frame_indices]\n        frames = np.stack(frames)\n        \n        if self.transform:\n            frames = [self.transform(frame) for frame in frames]\n            frames = torch.stack(frames)\n        \n        return frames, self.labels[idx]\n\n# Define transformations\ntransform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Create dataset and split\nfull_dataset = ViolenceDataset(data_root, transform=transform, num_frames=NUM_FRAMES)\nprint(len(full_dataset))\nif len(full_dataset) == 0:\n    raise ValueError(\"Dataset is empty. Please check the data directory structure and videos.\")\ntrain_size = int(SPLIT_RATIO * len(full_dataset))\nval_size = len(full_dataset) - train_size\ntrain_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n\n# Create dataloaders\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n\n# Define ViViT model\nclass ViViTClassifier(nn.Module):\n    def __init__(self, num_classes=2):\n        super(ViViTClassifier, self).__init__()\n        self.vivit = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\")\n        self.fc = nn.Linear(768, num_classes)\n    \n    def forward(self, x):\n        outputs = self.vivit(x).last_hidden_state[:, 0, :]\n        return self.fc(outputs)\n\n# Initialize model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = ViViTClassifier().to(device)\n\n# Define loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n\n# Training loop\nEPOCHS = 10\nfor epoch in range(EPOCHS):\n    model.train()\n    running_loss = 0.0\n    \n    for videos, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n        videos, labels = videos.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(videos)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    \n    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}\")\n    \nprint(\"Training complete!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T11:33:29.402906Z","iopub.execute_input":"2025-04-05T11:33:29.403220Z","iopub.status.idle":"2025-04-05T11:46:32.094607Z","shell.execute_reply.started":"2025-04-05T11:33:29.403196Z","shell.execute_reply":"2025-04-05T11:46:32.091490Z"}},"outputs":[{"name":"stdout","text":"350\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/725 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f56bbd575aab446cbad8630b7f222c80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/377M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6df6b8f3409347dc8722b24059c4f8fd"}},"metadata":{}},{"name":"stderr","text":"Epoch 1/10: 100%|██████████| 70/70 [05:56<00:00,  5.10s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Loss: 0.6386\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 70/70 [05:44<00:00,  4.93s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Loss: 0.4946\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10:  16%|█▌        | 11/70 [00:54<04:54,  4.98s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-1065b3afa87c>\u001b[0m in \u001b[0;36m<cell line: 101>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mvideos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{EPOCHS}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mvideos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-1065b3afa87c>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mtotal_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mframe_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_frames\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mframe_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-1065b3afa87c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mtotal_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mframe_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_frames\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mframe_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/decord/video_reader.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_frame\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Index: {} out of bound: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek_accurate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/decord/video_reader.py\u001b[0m in \u001b[0;36mseek_accurate\u001b[0;34m(self, pos)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0msuccess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_CAPI_VideoReaderSeekAccurate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Failed to seek_accurate to frame {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/decord/_ffi/_ctypes/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mret_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDECORDValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mret_tcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         check_call(_LIB.DECORDFuncCall(\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             ctypes.byref(ret_val), ctypes.byref(ret_tcode)))\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"def evaluate(model, dataloader, device):\n    model.eval()\n    correct = 0 \n    total = 0      \n    \n    with torch.no_grad():\n        for videos, labels in tqdm(dataloader, desc=\"Evaluating\"):\n            videos, labels = videos.to(device), labels.to(device)\n            outputs = model(videos)\n            _, predicted = torch.max(outputs, 1)\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n    \n    accuracy = correct / total * 100\n    print(f\"Test Accuracy: {accuracy:.2f}%\")\n    return accuracy\n\n# Create test dataloader\ntest_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n\n# Evaluate accuracy on test set\nevaluate(model, test_loader, device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T16:34:55.933487Z","iopub.execute_input":"2025-02-11T16:34:55.933801Z","iopub.status.idle":"2025-02-11T16:36:09.977193Z","shell.execute_reply.started":"2025-02-11T16:34:55.933775Z","shell.execute_reply":"2025-02-11T16:36:09.976543Z"}},"outputs":[{"name":"stderr","text":"Evaluating: 100%|██████████| 18/18 [01:14<00:00,  4.11s/it]","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 85.71%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"85.71428571428571"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"import os\nimport cv2\nimport torch\nimport random\nimport numpy as np\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import transforms, models\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom decord import VideoReader, cpu\nfrom tqdm import tqdm\n\n# Paths in Kaggle format\ndata_root = \"/kaggle/input/violence-detection-dataset/violence-detection-dataset\"\ncategories = [\"violent\", \"non-violent\"]\nSPLIT_RATIO = 0.8  # 80% train, 20% validation\nNUM_FRAMES = 16  # Number of frames per video clip\n\n# Define dataset class\nclass ViolenceDataset(Dataset):\n    def __init__(self, root_dir, transform=None, num_frames=16):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.num_frames = num_frames\n        self.videos = []\n        self.labels = []\n        \n        for category in categories:\n            category_path = os.path.join(root_dir, category)\n            label = 1 if category == \"violent\" else 0\n            for cam_folder in os.listdir(category_path):\n                cam_path = os.path.join(category_path, cam_folder)\n                if not os.path.isdir(cam_path):\n                    continue  # Skip if not a folder\n                for video_file in os.listdir(cam_path):\n                    if video_file.endswith(\".mp4\"):\n                        self.videos.append(os.path.join(cam_path, video_file))\n                        self.labels.append(label)\n    \n    def __len__(self):\n        return len(self.videos)\n    \n    def __getitem__(self, idx):\n        video_path = self.videos[idx]\n        vr = VideoReader(video_path, ctx=cpu(0))\n        total_frames = len(vr)\n        frame_indices = np.linspace(0, total_frames - 1, self.num_frames, dtype=int)\n        frames = [vr[i].asnumpy() for i in frame_indices]\n        frames = np.stack(frames)  # Shape: (T, H, W, C)\n        \n        if self.transform:\n            frames = [self.transform(frame) for frame in frames]\n            frames = torch.stack(frames)  # Shape: (T, C, H, W)\n        \n        frames = frames.permute(1, 0, 2, 3)  # Convert to (C, T, H, W)\n        \n        return frames, self.labels[idx]\n\n# Define transformations\ntransform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Create dataset and split\nfull_dataset = ViolenceDataset(data_root, transform=transform, num_frames=NUM_FRAMES)\nprint(len(full_dataset))\nif len(full_dataset) == 0:\n    raise ValueError(\"Dataset is empty. Please check the data directory structure and videos.\")\ntrain_size = int(SPLIT_RATIO * len(full_dataset))\nval_size = len(full_dataset) - train_size\ntrain_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n\n# Create dataloaders\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n\n# Define I3D model\nclass I3DClassifier(nn.Module):\n    def __init__(self, num_classes=2):\n        super(I3DClassifier, self).__init__()\n        self.i3d = models.video.r3d_18(pretrained=True)  # Use ResNet3D-18 as I3D variant\n        self.i3d.fc = nn.Linear(self.i3d.fc.in_features, num_classes)  # Adjust output layer\n    \n    def forward(self, x):\n        return self.i3d(x)\n\n# Initialize model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = I3DClassifier().to(device)\n\n# Define loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n\n# Training loop\nEPOCHS = 10\nfor epoch in range(EPOCHS):\n    model.train()\n    running_loss = 0.0\n    \n    for videos, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n        videos, labels = videos.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(videos)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    \n    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}\")\n    \nprint(\"Training complete!\")\n\ntorch.save(model.state_dict(), \"i3d_violence_detection.pth\")\nprint(\"Model saved as i3d_violence_detection.pth\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T11:46:41.568129Z","iopub.execute_input":"2025-04-05T11:46:41.568499Z","iopub.status.idle":"2025-04-05T12:40:00.203474Z","shell.execute_reply.started":"2025-04-05T11:46:41.568467Z","shell.execute_reply":"2025-04-05T12:40:00.202540Z"}},"outputs":[{"name":"stdout","text":"350\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=R3D_18_Weights.KINETICS400_V1`. You can also use `weights=R3D_18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/r3d_18-b3b3357e.pth\" to /root/.cache/torch/hub/checkpoints/r3d_18-b3b3357e.pth\n100%|██████████| 127M/127M [00:00<00:00, 214MB/s] \nEpoch 1/10: 100%|██████████| 70/70 [05:22<00:00,  4.61s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Loss: 0.4251\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 70/70 [05:20<00:00,  4.58s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Loss: 0.2855\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 70/70 [05:19<00:00,  4.56s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Loss: 0.1818\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|██████████| 70/70 [05:18<00:00,  4.55s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4, Loss: 0.1355\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|██████████| 70/70 [05:19<00:00,  4.57s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5, Loss: 0.1012\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|██████████| 70/70 [05:20<00:00,  4.58s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6, Loss: 0.1003\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|██████████| 70/70 [05:19<00:00,  4.57s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7, Loss: 0.0701\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|██████████| 70/70 [05:18<00:00,  4.55s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8, Loss: 0.1583\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|██████████| 70/70 [05:19<00:00,  4.56s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9, Loss: 0.0677\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 100%|██████████| 70/70 [05:18<00:00,  4.55s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 10, Loss: 0.0769\nTraining complete!\nModel saved as i3d_violence_detection.pth\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    confusion_matrix\n)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport torch\nfrom tqdm import tqdm\nimport numpy as np\n\n# Function to evaluate the model\ndef evaluate_model(model, dataloader, device, class_names=None):\n    model.eval()  # Set model to evaluation mode\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():  # No gradients needed for validation\n        for videos, labels in tqdm(dataloader, desc=\"Evaluating\"):\n            videos, labels = videos.to(device), labels.to(device)\n            \n            outputs = model(videos)\n            _, preds = torch.max(outputs, 1)  # Get class with highest probability\n            \n            all_preds.extend(preds.cpu().numpy())  # Convert to NumPy for metric calculation\n            all_labels.extend(labels.cpu().numpy())\n\n    # Compute metrics\n    acc = accuracy_score(all_labels, all_preds)\n    precision = precision_score(all_labels, all_preds, average='binary')\n    recall = recall_score(all_labels, all_preds, average='binary')\n    f1 = f1_score(all_labels, all_preds, average='binary')\n\n    print(f\"\\nAccuracy: {acc:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"F1 Score: {f1:.4f}\")\n\n    # Confusion matrix\n    cm = confusion_matrix(all_labels, all_preds)\n    plt.figure(figsize=(6, 5))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                xticklabels=class_names, yticklabels=class_names)\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Actual\")\n    plt.title(\"Confusion Matrix\")\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T12:48:03.639592Z","iopub.execute_input":"2025-04-05T12:48:03.639886Z","iopub.status.idle":"2025-04-05T12:48:03.914849Z","shell.execute_reply.started":"2025-04-05T12:48:03.639864Z","shell.execute_reply":"2025-04-05T12:48:03.914169Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class_names = ['Non-Violent', 'Violent']\n\n# Evaluate model with confusion matrix\nevaluate_model(model, val_loader, device, class_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T12:48:11.075687Z","iopub.execute_input":"2025-04-05T12:48:11.076218Z","iopub.status.idle":"2025-04-05T12:49:15.795860Z","shell.execute_reply.started":"2025-04-05T12:48:11.076193Z","shell.execute_reply":"2025-04-05T12:49:15.794975Z"}},"outputs":[{"name":"stderr","text":"Evaluating: 100%|██████████| 18/18 [01:04<00:00,  3.57s/it]\n","output_type":"stream"},{"name":"stdout","text":"\nAccuracy: 0.8429\nPrecision: 0.9535\nRecall: 0.8200\nF1 Score: 0.8817\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 600x500 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":8}]}